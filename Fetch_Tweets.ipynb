{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tweepy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the bearer token from the environment variables (required for v2)\n",
    "bearer_token = os.getenv('BEARER_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    client = tweepy.Client(bearer_token=bearer_token)\n",
    "    print(\"Authentication successful.\")\n",
    "except tweepy.TweepyException as e:\n",
    "    print(f\"Error during authentication: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search query\n",
    "query = 'Artificial Intelligence'\n",
    "# query = input(\"Enter Keywords for search query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 10 tweets.\n"
     ]
    }
   ],
   "source": [
    "# Fetch tweets using API v2\n",
    "try:\n",
    "    response = client.search_recent_tweets(query=query, max_results=10, tweet_fields=['text', 'created_at'])\n",
    "    \n",
    "    # Check if data is returned\n",
    "    if response.data:\n",
    "        tweets = response.data  # Store the fetched tweets\n",
    "        print(f\"Fetched {len(tweets)} tweets.\")\n",
    "    else:\n",
    "        print(\"No tweets found for this query.\")\n",
    "except tweepy.TweepyException as e:\n",
    "    print(f\"Error fetching tweets: {e}\")\n",
    "\n",
    "    # If rate-limited, inspect response headers for rate limit reset time\n",
    "    if hasattr(e, 'response') and 'x-rate-limit-reset' in e.response.headers:\n",
    "        reset_time = int(e.response.headers['x-rate-limit-reset'])\n",
    "        current_time = int(time.time())\n",
    "        wait_time = reset_time - current_time + 5 # Adding a 5 seconds buffer\n",
    "        print(f\"Rate limit exceeded. Try after {wait_time} seconds.\")\n",
    "        time.sleep(wait_time)  # Sleep until rate limit resets\n",
    "    else:\n",
    "        print(\"Unhandled error. Please check API access or query.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Tweet id=1856927494047240488 text='@DeSi__kaTTa AI-Powered Art is an innovative fusion of technology and creativity, where artificial intelligence algorithms assist in generating, enhancing, or even creating original pieces of artwork.'>, <Tweet id=1856927484962300385 text='@DeSi__kaTTa AI-powered art encompasses a range of artistic practices where artificial intelligence technologies are used to create, enhance, or inspire visual artworks'>, <Tweet id=1856927374119510412 text='A decentralized delivery platform powered by blockchain and artificial intelligence has the potential to revolutionize the sector by fostering a more just and efficient ecosystem. https://t.co/le55GcJPWW'>, <Tweet id=1856927358642479570 text='RT @andrewrsorkin: Ok. So this happened. \\n\\nIt is NOT us at all. \\n\\nFully generated by artificial intelligence. \\n\\nThe video, audio. All of it…'>, <Tweet id=1856927350576853387 text='I can\\'t wait for AI to really become artificial intelligence so when people ask bullshit prompts like this the ai will just be like \"what the fuck did you just say to me\" https://t.co/TSibz05vVb'>, <Tweet id=1856927297225297939 text='RT @AndrewColorzz: Artificial &amp; Biological Intelligence visiting earth🌎🤔\\n#UAPHearing https://t.co/gw6pFOWVAd'>, <Tweet id=1856927254212722754 text='RT @EU_Commission: AI in Europe: safe, trustworthy, and human-centric.\\n\\nWe are inviting feedback to prepare new guidelines that will help d…'>, <Tweet id=1856927163976716744 text=\"@MEXC_Official @zulu_network With @zulu_network focusing on AI+ DePIN, how do you see Bitcoin's role evolving in artificial intelligence applications, particularly in terms of computation and data storage?\">, <Tweet id=1856927152945459657 text='RT @scribe9104: Artificial Intelligence की मदद से राजा रवि वर्मा की मशहूर पेंटिंगों में जान फूंकी गयी है \\n\\nज़बरदस्त 🥰\\n https://t.co/U2Crc3T…'>, <Tweet id=1856927122016932057 text='How many users on social media, including on X, are artificial intelligence (computer bots)???\\n\\nWATCH OUT for “who” influences you!!!\\n\\nLin 🙏❤️🇺🇸\\nhttps://t.co/uEz30RCuDD\\n\\n#DONOTLETAIFOOLYOU\\n\\nhttps://t.co/4gFEf88Hjf\\nhttps://t.co/z2F9Md1zpb\\nDoes God expect us to have blind faith? https://t.co/3JAlx99BJk'>]\n"
     ]
    }
   ],
   "source": [
    "print(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Preproccessing and cleaning the extracted tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from langdetect import detect\n",
    "from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/divyagoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/divyagoyal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divyagoyal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/divyagoyal/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download the required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess tweet\n",
    "spell = Speller()\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove Retweets (RT)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "\n",
    "    # Remove mentions (e.g., @username)\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)\n",
    "\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+', '', tweet)\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    tweet = re.sub(r'[^A-Za-z\\s]', '', tweet)\n",
    "\n",
    "    # Correct spelling mistakes using spell checker\n",
    "    tweet = ' '.join([spell(word) for word in tweet.split()])\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and preprocess tweets\n",
    "cleaned_tweets = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for tweet in tweets:\n",
    "    # Clean each tweet and append to the list\n",
    "    cleaned_tweet = preprocess_tweet(tweet.text)\n",
    "    cleaned_tweets.append(cleaned_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powered art innovative fusion technology creativity artificial intelligence algorithms assist generating enhancing even creating original pieces artwork\n",
      "powered art encompasses range artistic practices artificial intelligence technologies used create enhance inspire visual artworks\n",
      "decentralized delivery platform powered blockchain artificial intelligence potential revolutionized sector fostering efficient ecosystem\n",
      "ok happened us fully generated artificial intelligence video audio\n",
      "cant wait really become artificial intelligence people ask bullshit prompts like ai like fuck say\n",
      "artificial amp biological intelligence visiting earth uaphearing\n",
      "europe safe trustworthy humancentric inviting feedback prepare new guidelines help\n",
      "focusing pin see bitcoin role evolving artificial intelligence applications particularly terms computation data storage\n",
      "artificial intelligence\n",
      "many users social media including x artificial intelligence computer bots tch ut influences lin donotletaifoolyou god expect us blind faith\n"
     ]
    }
   ],
   "source": [
    "# Print cleaned and tokenized tweets\n",
    "for tweet in cleaned_tweets:\n",
    "    print(' '.join(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performing Sentiment Analysis: Using TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./senv/lib/python3.12/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in ./senv/lib/python3.12/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in ./senv/lib/python3.12/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./senv/lib/python3.12/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./senv/lib/python3.12/site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in ./senv/lib/python3.12/site-packages (from nltk>=3.8->textblob) (4.67.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment\n",
    "def get_sentiment(text):\n",
    "    # Create a TextBlob object\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get the polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "    sentiment = blob.sentiment\n",
    "    return sentiment.polarity, sentiment.subjectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis on cleaned tweets\n",
    "for tweet in cleaned_tweets:\n",
    "    # Join tokens back to form tweet text\n",
    "    tweet_text = ' '.join(tweet)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment\n",
    "polarity, subjectivity = get_sentiment(tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: many users social media including x artificial intelligence computer bots tch ut influences lin donotletaifoolyou god expect us blind faith\n",
      "Polarity: -0.14166666666666666, Subjectivity: 0.5583333333333333\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Print the results\n",
    "print(f\"Tweet: {tweet_text}\")\n",
    "print(f\"Polarity: {polarity}, Subjectivity: {subjectivity}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
